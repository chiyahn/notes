---
title: "Text mining with `rvest`"
author: "Chiyoung Ahn"
date: "October 31, 2018"
output:
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Goals:
- extract an abstract of a journal article available in AEA website

# Text mining with `rvest`
Unfortunately, to access a journal article with URL, you need DOI and AER identifier. For instance, the URL for one of my favourite articles (Partner Choice, Investment in Children, and the Marital College Premium) by Chiappori et al. (2017) is `https://www.aeaweb.org/articles?id=10.1257/aer.20150154` where `10.1257` is DOI and `20150154` is AER identifier.

One good news is that AEA website has a user friendly URL protocol for each *issue* that was published after the winter of 2002 -- it is not difficult to see that each issue is saved as `https://www.aeaweb.org/issues/ISSUE.NUMBER` where `ISSUE.NUMBER` increases by one whenever there is a next issue for one of AER/AERInsights/AEJApplied/AEJPolicy/AEJMacro/AEJMicro/JEL/JEP. Note that the journal issue corresponding to `ISSUE.NUMBER = 1` is JEP Vol. 16 No. 1 published in Winter 2002. For expository purpose, let's focus on the journal issue with `ISSUE.NUMBER = 475` (AER Vol.107 No.8) for now. You can iterate over all issues or limit the journals of interest to one particular journal if you want.

## Extracting article names and URLs from a single issue
The following script extracts the HTML of the journal issue with `ISSUE.NUMBER = 475`:
```{r extract-html}
# install.packages("rvest") # uncomment and run this line if rvest is not installed
# install.packages("stringi") # uncomment and run this line if stringi is not installed
library(rvest)
library(stringi)

ISSUE.NUMBER <- 475
issue.html <- read_html(paste0("https://www.aeaweb.org/issues/", ISSUE.NUMBER))
```

It's time to use SelectorGadget ([https://selectorgadget.com/](https://selectorgadget.com/)) to figure out how each webpage was constructed. It is not difficult to see that
- node with `section h1` contains the title of the journal (`American Economic Review`)
- node with `section h2` contains the volume number and date of publication (`Vol. 107 No. 8 August 2017`)
- nodes with `h3 a` contains the titles of all articles appeared in the issue (`Front Matter`, `Partner Choice, Investment in Children, and the Marital College Premium`, ...)

### Journal name
To extract the journal name, 
```{r extract-title}
journal.name <- issue.html %>% 
  html_nodes("section h1") %>% # access the html node with `section h1`
  html_text() # filter the html data so that we have only string data

print(journal.name)
```

### Year of publication
Similarly, we can extract the year of publication.
```{r extract-year}
year.of.pub <- issue.html %>% 
  html_nodes("section h2") %>% # access the html node with `section h2`
  html_text() # filter the html data so that we have only string data

print(year.of.pub) # this line contains volume #/month/year. Extract the very last word for year!

```
Note that the text we extracted contains volume number AND month of publication AND year of publication. We can use `stringr` to extract the very last word, which is the year of publication.
```{r extract-year2}
year.of.pub <- stri_extract_last_words(year.of.pub) # extract the last word
print(year.of.pub)
```

### Article titles
Similarly, for article titles:
```{r}
article.titles <- issue.html %>% 
  html_nodes("h3 a") %>% # access the html node with `h3 a`
  html_text() # filter the html data so that we have only string data

print(article.titles)
```

### Article URLs
Now it's time to grab article URLs from each issue! To do so, simply change `html_text()` in piping to `html_attr("href")` since we are looking for the URL that each journal article links to.

```{r}
article.urls <- issue.html %>% 
  html_nodes("h3 a") %>% # access the html node with `h3 a`
  html_attr("href") # filter the html data so that we have only URL string

print(article.urls)
```

Note that the URLs we get are not the full URLs (which we need to get access to the HTML data), rather the URLs relative to the home website (`https://www.aeaweb.org/`). Hence, to get the complete URLs, we have to put `https://www.aeaweb.org` in front. To do so, use `paste0` in R. Note that `paste0` can be applied to vector too.

```{r}
article.urls <- paste0("https://www.aeaweb.org", article.urls)
article.urls
```
### Extracting abstract
Let's read the abstract:
```{r}
(read_html(article.urls[2]) %>%
  html_nodes("section section") %>%
  html_text())[3]
```



